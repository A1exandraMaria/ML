# Importing necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

# Defining the neural network model class
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()  # Initializing the base class nn.Module
        # First convolutional layer taking 3 input channels, returning 6 output channels, kernel size 5x5
        self.conv1 = nn.Conv2d(3, 6, 5)
        # Max Pooling layer that reduces image size by half
        self.pool = nn.MaxPool2d(2, 2)
        # Second convolutional layer, 6 input channels, 16 output channels, kernel size 5x5
        self.conv2 = nn.Conv2d(6, 16, 5)
        # Three fully connected layers
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Processes vector from size 16*5*5 to size 120
        self.fc2 = nn.Linear(120, 84)  # Processes vector from size 120 to size 84
        self.fc3 = nn.Linear(84, 10)  # Processes vector from size 84 to size 10 (number of classes)

    def forward(self, x):
        # Defining data flow through the network
        x = self.pool(F.relu(self.conv1(x)))  # ReLU activation and pooling for the first convolutional layer
        x = self.pool(F.relu(self.conv2(x)))  # ReLU activation and pooling for the second convolutional layer
        x = x.view(-1, 16 * 5 * 5)  # Flattening the tensor into a vector
        x = F.relu(self.fc1(x))  # First fully connected layer and ReLU activation
        x = F.relu(self.fc2(x))  # Second fully connected layer and ReLU activation
        x = self.fc3(x)  # Output layer
        return x

# Initial settings
batch_size = 64  # Number of samples in one batch
epochs = 2  # Number of training epochs
learning_rate = 0.001  # Learning rate

# Preparing data: transforming and loading the CIFAR-10 dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])  # Data transformations
trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)  # Training dataset
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)  # Training data loader

# Model, loss function, and optimizer initialization
net = SimpleCNN()  # Model instance
criterion = nn.CrossEntropyLoss()  # Loss function (cross entropy)
optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)  # Optimizer (SGD with momentum)

# Training loop
for epoch in range(epochs):  # For each epoch...
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):  # For each batch...
        inputs, labels = data  # Unpacking data into inputs and labels
        optimizer.zero_grad()  # Zeroing the gradients
        outputs = net(inputs)  # Passing data through the network
        loss = criterion(outputs, labels)  # Calculating the loss
        loss.backward()  # Backpropagation
        optimizer.step()  # Updating the weights
        running_loss += loss.item()  # Summing up the loss
        if i % 2000 == 1999:  # Printing average loss every 2000 batches
            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
            running_loss = 0.0

# Training completion message
print('Finished Training')
